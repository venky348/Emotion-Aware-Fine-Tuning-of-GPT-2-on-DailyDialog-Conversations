{"cells":[{"cell_type":"markdown","metadata":{"id":"ZxoGKhTdVU_S"},"source":["# 1"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HAHeG5O1Va8-","executionInfo":{"status":"ok","timestamp":1734371903297,"user_tz":300,"elapsed":759,"user":{"displayName":"Venkatesh Talasila","userId":"06752232086530326792"}},"outputId":"0ffce32d-561f-4a61-af61-ed9473af6f2c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"iu99NlmkVU_U"},"source":["## Dataset Description: **DailyDialog**\n","\n","**Source**: [DailyDialog Dataset on Kaggle](https://www.kaggle.com/datasets/va6573/dailydialog)\n","\n","The **DailyDialog** dataset consists of daily conversations, making it suitable for training models on dialogue-based tasks. The dataset is specifically designed for emotion detection in dialogues, containing conversations across various emotional contexts.\n","\n","## Key Information:\n","\n","- **Purpose**: The dataset is used for training models to recognize emotions in text-based dialogues.\n","  \n","- **Content**:\n","  - The dataset contains conversations where each dialogue is associated with an emotion label.\n","  - The emotions in the dataset include common categories such as **neutral**, **joy**, and others (e.g., sadness, anger).\n","\n","- **File Structure**:\n","  - **daily_dialog_train_cleaned.csv**: The training set with dialogues and corresponding emotion labels.\n","  - **daily_dialog_val_cleaned.csv**: The validation set to evaluate the model during training.\n","  - **daily_dialog_test_cleaned.csv**: The test set for final model evaluation.\n","\n","- **Columns**:\n","  - **Text**: The dialogue text in the conversation.\n","  - **Emotion**: The label representing the emotion in the dialogue (e.g., neutral, joy).\n","\n","- **Emotion Distribution**:\n","  - **Neutral**: 83% of the dataset\n","  - **Joy**: 13% of the dataset\n","  - **Other Emotions**: 4% of the dataset\n","\n","- **Size**:\n","  - The dataset contains **9,624** dialogues in total.\n","\n","## Example Entries:\n","\n","- **Neutral**:\n","  - *\"I really think you are stubborn about some things, but here let us look at the new balance shoes.\"*\n","  - *\"Where else have not I been to yet?\"*\n","\n","- **Joy**:\n","  - *\"Um well actually we had a fantastic time last night, he was amazing.\"*\n","  - *\"No, I am free. I will be there. What time is the thing starting?\"*\n","\n","## Use Case:\n","This dataset is ideal for training dialogue systems or models for tasks such as:\n","- **Emotion detection in conversations**: Predict the emotional state of the speaker from the dialogue text.\n","- **Sentiment analysis**: Classify text based on emotional tone.\n","  \n","It can be used for various natural language processing (NLP) applications, including chatbots, virtual assistants, and sentiment analysis tools.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"IxWs_09bVU_V","executionInfo":{"status":"ok","timestamp":1734371921704,"user_tz":300,"elapsed":18412,"user":{"displayName":"Venkatesh Talasila","userId":"06752232086530326792"}}},"outputs":[],"source":["from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments\n","from datasets import Dataset, DatasetDict\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"nFhGmksNVU_W","executionInfo":{"status":"ok","timestamp":1734371921959,"user_tz":300,"elapsed":260,"user":{"displayName":"Venkatesh Talasila","userId":"06752232086530326792"}}},"outputs":[],"source":["# Load dataset\n","train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/daily_dialog_train_cleaned.csv')\n","val_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/daily_dialog_val_cleaned.csv')\n","test_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/daily_dialog_test_cleaned.csv')\n","\n","# Replace missing values in the 'Text' column with an empty string or placeholder\n","train_df[\"Text\"] = train_df[\"Text\"].fillna(\"\")\n","val_df[\"Text\"] = val_df[\"Text\"].fillna(\"\")\n","test_df[\"Text\"] = test_df[\"Text\"].fillna(\"\")\n","\n","# Take a subset of the training and validation datasets\n","train_df = train_df.sample(n=10000, random_state=42)  # 10,000 samples for training\n","val_df = val_df.sample(n=2000, random_state=42) # 2,000 samples for validation"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"K3CPrc0rVU_W","executionInfo":{"status":"ok","timestamp":1734371921960,"user_tz":300,"elapsed":9,"user":{"displayName":"Venkatesh Talasila","userId":"06752232086530326792"}}},"outputs":[],"source":["# Step 3: Label Encoding (Emotion labels)\n","label_encoder = LabelEncoder()\n","\n","# Fit label encoder on the training data's Emotion column and transform it\n","train_df['label'] = label_encoder.fit_transform(train_df['Emotion'])\n","val_df['label'] = label_encoder.transform(val_df['Emotion'])\n","test_df['label'] = label_encoder.transform(test_df['Emotion'])"]},{"cell_type":"markdown","metadata":{"id":"K_WdfCdFVU_X"},"source":["# 2"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"07RPDE_xVU_X","executionInfo":{"status":"ok","timestamp":1734371921961,"user_tz":300,"elapsed":10,"user":{"displayName":"Venkatesh Talasila","userId":"06752232086530326792"}}},"outputs":[],"source":["# Convert to Hugging Face Dataset format\n","train_dataset = Dataset.from_pandas(train_df[['Text']])\n","val_dataset = Dataset.from_pandas(val_df[['Text']])\n","test_dataset = Dataset.from_pandas(test_df[['Text']])"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4kea18fSVU_X","executionInfo":{"status":"ok","timestamp":1734371923618,"user_tz":300,"elapsed":1666,"user":{"displayName":"Venkatesh Talasila","userId":"06752232086530326792"}},"outputId":"c248704c-bc42-4d74-ac46-e7a979b5c497"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n","\n","# Load GPT-2 tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n","\n","# Fix the padding issue by using the eos_token as pad_token\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["1052b639c8de453f97b5a2edc2b0b8c9","da64802b50f64a78998e642ea520d053","4a8d3f0ac05841a08c8711271a14fbcc","1cd447e17a2249e88d672a2f11c5f667","0f2c99172d0740c18b46e3d1924a061f","3b659fdf148a4709b890f285dca5934a","72c259b2608d4ed9b223b1657bf07078","83f8f0430d1345f5948164dc4322e1d5","479bde50eed642ddb82b7c090bb205df","5e390b523b4845ddab7a3cfb864a6080","9a477873146a4e9fb4e3c821a8391515","cbc170c5e6d54d27a79acb9d707c5313","39c18ee18666480b9063a2f429ad86e1","35e70d5c3d0e4fdc994bbf02d33d341b","fdbcba2c50694239ad191b73ec6f8ff3","e887f6e226854096879c9e3c7c77b5ed","db5985aca30741f281ef47779b0bf7de","ca64978baba5480587c31408cc023422","5ca02a46d4f64f8d9435110b619a9ed7","f542aad760024132880abed79068cdb6","19462952619d49918da023efbf4deaf3","65ab0205f6de420cb53ed4eef5999f2b","ba377c7677824486b51e27ca85950595","68db64af24a84610a8ecd15417538850","77fbfd54d8924b93b78c66e0ffab9e68","a4397f39a805455694ac2ce39f3efb37","5ad427b6e2f34b5a909f811457df17bd","38627d75bbd148fa908886ebe4b6ec32","abf6783f7f684768abf419710da23c0f","d5a39cfcebc94d0fa6c99aeece8e359e","cbab3cb5edf04befaca8461052dbd75e","50b17c15165c4ce397ad16870e855c57","c6af822c6de6476e9aa55ee8128dc791"]},"id":"pfcMhSl-VU_X","executionInfo":{"status":"ok","timestamp":1734371931522,"user_tz":300,"elapsed":7910,"user":{"displayName":"Venkatesh Talasila","userId":"06752232086530326792"}},"outputId":"b69fa74b-aab6-48f1-9997-cda3467ebdb1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1052b639c8de453f97b5a2edc2b0b8c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbc170c5e6d54d27a79acb9d707c5313"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/10298 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba377c7677824486b51e27ca85950595"}},"metadata":{}}],"source":["def tokenize_function(examples):\n","    # Tokenize the text and create input_ids and labels\n","    tokenized = tokenizer(\n","        examples[\"Text\"],  # Column name from the dataset\n","        truncation=True,\n","        padding=\"max_length\",\n","        max_length=128\n","    )\n","    # Labels are identical to input_ids for causal language modeling\n","    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n","    return tokenized\n","\n","# Tokenize train and validation datasets\n","train_dataset = train_dataset.map(tokenize_function, batched=True)\n","val_dataset = val_dataset.map(tokenize_function, batched=True)\n","test_dataset = test_dataset.map(tokenize_function, batched=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"HyXBojX7VU_Y","executionInfo":{"status":"ok","timestamp":1734371931523,"user_tz":300,"elapsed":7,"user":{"displayName":"Venkatesh Talasila","userId":"06752232086530326792"}}},"outputs":[],"source":["# Set the format for PyTorch (required for Trainer)\n","train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"]},{"cell_type":"code","source":["import wandb\n","from transformers import Trainer, TrainingArguments\n","\n","# Initialize wandb\n","wandb.init(\n","    project=\"AML_HW_4_2\",  # Name your project here\n","    config={\n","        \"learning_rate\": 2e-5,\n","        \"epochs\": 3,\n","        \"batch_size\": 16,\n","    }\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":558},"id":"pdctVYrPnN5I","executionInfo":{"status":"ok","timestamp":1734371934982,"user_tz":300,"elapsed":3465,"user":{"displayName":"Venkatesh Talasila","userId":"06752232086530326792"}},"outputId":"fb1d3ef3-acb5-4f0a-c06a-0ef4d6e4cbd3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtalasilavenkatesh2\u001b[0m (\u001b[33mtalasilavenkatesh2-indiana-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.18.7"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20241216_175852-6d30pivr</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/talasilavenkatesh2-indiana-university/AML_HW_4_2/runs/6d30pivr' target=\"_blank\">dutiful-capybara-1</a></strong> to <a href='https://wandb.ai/talasilavenkatesh2-indiana-university/AML_HW_4_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/talasilavenkatesh2-indiana-university/AML_HW_4_2' target=\"_blank\">https://wandb.ai/talasilavenkatesh2-indiana-university/AML_HW_4_2</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/talasilavenkatesh2-indiana-university/AML_HW_4_2/runs/6d30pivr' target=\"_blank\">https://wandb.ai/talasilavenkatesh2-indiana-university/AML_HW_4_2/runs/6d30pivr</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/talasilavenkatesh2-indiana-university/AML_HW_4_2/runs/6d30pivr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x793bc5562e90>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311},"id":"Pg5GecLyVU_Y","executionInfo":{"status":"ok","timestamp":1734373107086,"user_tz":300,"elapsed":1172107,"user":{"displayName":"Venkatesh Talasila","userId":"06752232086530326792"}},"outputId":"aa2db0fd-5a99-431e-ae45-6574b903de41"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-10-339f33d74f81>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1875/1875 19:28, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.368400</td>\n","      <td>0.347547</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.347500</td>\n","      <td>0.341447</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.332100</td>\n","      <td>0.340335</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=1875, training_loss=0.37476914825439456, metrics={'train_runtime': 1170.8772, 'train_samples_per_second': 25.622, 'train_steps_per_second': 1.601, 'total_flos': 1959690240000000.0, 'train_loss': 0.37476914825439456, 'epoch': 3.0})"]},"metadata":{},"execution_count":10}],"source":["from transformers import Trainer, TrainingArguments\n","\n","# Step 5: Define the training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",  # Directory to save the model\n","    evaluation_strategy=\"epoch\",  # Evaluate the model after every epoch\n","    save_strategy=\"epoch\",  # Save the model after every epoch\n","    learning_rate=2e-5,  # Learning rate\n","    per_device_train_batch_size=16,  # Batch size for training\n","    per_device_eval_batch_size=16,  # Batch size for evaluation\n","    num_train_epochs=3,  # Number of training epochs\n","    weight_decay=0.01,  # Weight decay to avoid overfitting\n","    logging_dir=\"./logs\",  # Directory to save logs\n","    logging_steps=10,  # Log every 10 steps\n","    push_to_hub=False  # Set to True if pushing to Hugging Face Model Hub\n",")\n","\n","# Step 6: Set up the Trainer\n","trainer = Trainer(\n","    model=model,  # The GPT-2 model\n","    args=training_args,  # Training arguments\n","    train_dataset=train_dataset,  # Training dataset\n","    eval_dataset=val_dataset,  # Validation dataset\n","    tokenizer=tokenizer  # Tokenizer for data processing\n",")\n","\n","# Step 7: Train the model\n","trainer.train()"]},{"cell_type":"code","source":["# Evaluate on the test dataset\n","metrics = trainer.evaluate(test_dataset)\n","print(metrics)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"mQvN5aXye3ot","executionInfo":{"status":"ok","timestamp":1734373214551,"user_tz":300,"elapsed":107482,"user":{"displayName":"Venkatesh Talasila","userId":"06752232086530326792"}},"outputId":"6238fa6c-72a9-414e-a65c-8af35b20562b"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='644' max='644' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [644/644 01:47]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.33578893542289734, 'eval_runtime': 107.3908, 'eval_samples_per_second': 95.893, 'eval_steps_per_second': 5.997, 'epoch': 3.0}\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hUquhxDnVU_Y","executionInfo":{"status":"ok","timestamp":1734373223557,"user_tz":300,"elapsed":9013,"user":{"displayName":"Venkatesh Talasila","userId":"06752232086530326792"}},"outputId":"69c03154-d6d7-4662-b875-1b459a72f012"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/MyDrive/Colab Notebooks/data/fine_tuned_gpt2/tokenizer_config.json',\n"," '/content/drive/MyDrive/Colab Notebooks/data/fine_tuned_gpt2/special_tokens_map.json',\n"," '/content/drive/MyDrive/Colab Notebooks/data/fine_tuned_gpt2/vocab.json',\n"," '/content/drive/MyDrive/Colab Notebooks/data/fine_tuned_gpt2/merges.txt',\n"," '/content/drive/MyDrive/Colab Notebooks/data/fine_tuned_gpt2/added_tokens.json',\n"," '/content/drive/MyDrive/Colab Notebooks/data/fine_tuned_gpt2/tokenizer.json')"]},"metadata":{},"execution_count":12}],"source":["# Step 8: Save the trained model\n","trainer.save_model(\"/content/drive/MyDrive/Colab Notebooks/data/fine_tuned_gpt2\")\n","tokenizer.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/data/fine_tuned_gpt2\")"]},{"cell_type":"markdown","source":["## Afer Finetuning the model"],"metadata":{"id":"HmW3Ky5YkgE2"}},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","# Load the fine-tuned model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/data/fine_tuned_gpt2\")\n","tokenizer = GPT2Tokenizer.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/data/fine_tuned_gpt2\")\n","\n","# Generate responses for custom prompts\n","prompts = [\n","    \"congratulations\",\n","    \"good afternoon madam\",\n","]\n","\n","for prompt in prompts:\n","    inputs = tokenizer(prompt, return_tensors=\"pt\")\n","    outputs = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=5, early_stopping=True)\n","    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"voXtYVFrf2er","executionInfo":{"status":"ok","timestamp":1734373225760,"user_tz":300,"elapsed":2206,"user":{"displayName":"Venkatesh Talasila","userId":"06752232086530326792"}},"outputId":"641580e0-3c2d-4bc1-c9e9-909c08013ba4"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["congratulations to you sir\n","good afternoon madam i am sorry for the delay\n"]}]},{"cell_type":"markdown","source":["## Before Fine Tuning the model."],"metadata":{"id":"33Isu2qYkb3x"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Load the fine-tuned model and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Replace with the path to your fine-tuned model\n","model = AutoModelForCausalLM.from_pretrained(\"gpt2\")  # Replace with the path to your fine-tuned model\n","\n","# Set pad_token to eos_token (since GPT-2 doesn't have a dedicated padding token)\n","tokenizer.pad_token = tokenizer.eos_token  # Setting the pad token to eos_token\n","model.config.pad_token_id = model.config.eos_token_id  # Ensuring the model config uses eos_token for padding\n","\n","# Define the prompt\n","prompt = \"Congratulations\"\n","\n","# Tokenize input with padding and attention mask\n","inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=50)\n","\n","# Generate text using the model, ensuring attention mask is passed\n","outputs = model.generate(\n","    inputs[\"input_ids\"],\n","    max_length=50,\n","    num_return_sequences=1,\n","    attention_mask=inputs[\"attention_mask\"],  # Pass attention mask to avoid padding issues\n","    top_p=0.95,  # Nucleus sampling for more varied generation\n","    temperature=0.7,  # Temperature setting to introduce randomness into generation\n","    do_sample=True  # Enable sampling to make use of top_p and temperature\n",")\n","\n","# Decode and print the generated text\n","generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(generated_text)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JFERdmVEkaB1","executionInfo":{"status":"ok","timestamp":1734373228387,"user_tz":300,"elapsed":2633,"user":{"displayName":"Venkatesh Talasila","userId":"06752232086530326792"}},"outputId":"4fd8c087-d201-4df4-e2c7-e008576969ef"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Congratulations as you do. I'd like to thank you for your time, and I hope you'll be able to share more of this information with your family, friends and colleagues.\n"]}]},{"cell_type":"markdown","source":["### Observations:\n","\n","#### **Before Fine-tuning:**\n","- The model's output was not very relevant to the prompt.\n","- It seemed to pull random content or context that wasn't aligned with the input, such as references to forums, websites, and disjointed responses.\n","\n","#### **After Fine-tuning:**\n","- The outputs are much more contextually aligned with the input prompt.\n","- For example, the response to \"congratulations\" is now a more suitable and relevant phrase like \"congratulations to you sir,\" and the response to \"good afternoon madam\" is more coherent with \"good afternoon madam, I am sorry for the delay.\"\n","- Fine-tuning has clearly helped the model generate responses that are more in line with the desired tone and context for conversational text.\n","\n","---\n","\n","### **Conclusion:**\n","- Fine-tuning the model on your specific dataset has led to a more appropriate and focused response for conversational prompts, moving away from the random, less relevant outputs before fine-tuning. This aligns the model with the language patterns and behavior that you aimed to train it on.\n","\n","- The interesting difference observed is that after fine-tuning, the model's responses are **more coherent, polite, and contextually appropriate**, in contrast to the disjointed and seemingly random outputs it produced before fine-tuning.\n","\n","#### **If you didn't observe a similar difference, there could be several factors to check:**\n","- **Insufficient training data:** The dataset may not have been large or diverse enough to allow the model to generalize well.\n","- **Training duration:** If the model wasn't trained for long enough, it may not have had sufficient time to learn the desired patterns.\n","- **Model size and complexity:** If using a smaller model or suboptimal configurations, the fine-tuning might not have led to a significant improvement in output quality.\n"],"metadata":{"id":"3k3gEf-iljSk"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.20"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1052b639c8de453f97b5a2edc2b0b8c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da64802b50f64a78998e642ea520d053","IPY_MODEL_4a8d3f0ac05841a08c8711271a14fbcc","IPY_MODEL_1cd447e17a2249e88d672a2f11c5f667"],"layout":"IPY_MODEL_0f2c99172d0740c18b46e3d1924a061f"}},"da64802b50f64a78998e642ea520d053":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b659fdf148a4709b890f285dca5934a","placeholder":"​","style":"IPY_MODEL_72c259b2608d4ed9b223b1657bf07078","value":"Map: 100%"}},"4a8d3f0ac05841a08c8711271a14fbcc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_83f8f0430d1345f5948164dc4322e1d5","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_479bde50eed642ddb82b7c090bb205df","value":10000}},"1cd447e17a2249e88d672a2f11c5f667":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e390b523b4845ddab7a3cfb864a6080","placeholder":"​","style":"IPY_MODEL_9a477873146a4e9fb4e3c821a8391515","value":" 10000/10000 [00:03&lt;00:00, 1915.98 examples/s]"}},"0f2c99172d0740c18b46e3d1924a061f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b659fdf148a4709b890f285dca5934a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72c259b2608d4ed9b223b1657bf07078":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83f8f0430d1345f5948164dc4322e1d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"479bde50eed642ddb82b7c090bb205df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5e390b523b4845ddab7a3cfb864a6080":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a477873146a4e9fb4e3c821a8391515":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbc170c5e6d54d27a79acb9d707c5313":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_39c18ee18666480b9063a2f429ad86e1","IPY_MODEL_35e70d5c3d0e4fdc994bbf02d33d341b","IPY_MODEL_fdbcba2c50694239ad191b73ec6f8ff3"],"layout":"IPY_MODEL_e887f6e226854096879c9e3c7c77b5ed"}},"39c18ee18666480b9063a2f429ad86e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db5985aca30741f281ef47779b0bf7de","placeholder":"​","style":"IPY_MODEL_ca64978baba5480587c31408cc023422","value":"Map: 100%"}},"35e70d5c3d0e4fdc994bbf02d33d341b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ca02a46d4f64f8d9435110b619a9ed7","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f542aad760024132880abed79068cdb6","value":2000}},"fdbcba2c50694239ad191b73ec6f8ff3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19462952619d49918da023efbf4deaf3","placeholder":"​","style":"IPY_MODEL_65ab0205f6de420cb53ed4eef5999f2b","value":" 2000/2000 [00:00&lt;00:00, 3812.66 examples/s]"}},"e887f6e226854096879c9e3c7c77b5ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db5985aca30741f281ef47779b0bf7de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca64978baba5480587c31408cc023422":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ca02a46d4f64f8d9435110b619a9ed7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f542aad760024132880abed79068cdb6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"19462952619d49918da023efbf4deaf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65ab0205f6de420cb53ed4eef5999f2b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba377c7677824486b51e27ca85950595":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_68db64af24a84610a8ecd15417538850","IPY_MODEL_77fbfd54d8924b93b78c66e0ffab9e68","IPY_MODEL_a4397f39a805455694ac2ce39f3efb37"],"layout":"IPY_MODEL_5ad427b6e2f34b5a909f811457df17bd"}},"68db64af24a84610a8ecd15417538850":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38627d75bbd148fa908886ebe4b6ec32","placeholder":"​","style":"IPY_MODEL_abf6783f7f684768abf419710da23c0f","value":"Map: 100%"}},"77fbfd54d8924b93b78c66e0ffab9e68":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5a39cfcebc94d0fa6c99aeece8e359e","max":10298,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cbab3cb5edf04befaca8461052dbd75e","value":10298}},"a4397f39a805455694ac2ce39f3efb37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50b17c15165c4ce397ad16870e855c57","placeholder":"​","style":"IPY_MODEL_c6af822c6de6476e9aa55ee8128dc791","value":" 10298/10298 [00:03&lt;00:00, 3488.22 examples/s]"}},"5ad427b6e2f34b5a909f811457df17bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38627d75bbd148fa908886ebe4b6ec32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abf6783f7f684768abf419710da23c0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5a39cfcebc94d0fa6c99aeece8e359e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbab3cb5edf04befaca8461052dbd75e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"50b17c15165c4ce397ad16870e855c57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6af822c6de6476e9aa55ee8128dc791":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}